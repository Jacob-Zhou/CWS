[Run]
is_test = 0
is_train = 1
device-x = cpu
device = cuda:0
cpu_thread_num = 8
dict_dir = ../save/
word_freq_cutoff = 1
model_dir = ../save/
inst_num_max = -1
sent_max_len = 100
max_bucket_num = 80
sent_num_one_batch = 50
word_num_one_batch = 5000

[Test]
model_eval_num = 0

[Train]
data_dir = ../data
train_files = %(data_dir)s/ctb5/train.conll
dev_files = %(data_dir)s/ctb5/dev.conll
test_files = %(data_dir)s/ctb5/test.conll
is_dictionary_exist = 1
train_max_eval_num = 1000
save_model_after_eval_num = 50
train_stop_after_eval_num_no_improve = 200
eval_every_update_step_num = 136

[Network]
lstm_layer_num = 3
char_emb_dim = 100
emb_dropout_ratio = 0.33
lstm_hidden_dim = 400
lstm_dropout_ratio = 0.55
lstm_input_dropout_ratio = 0.33
lstm_hidden_dropout_ratio_for_next_timestamp = 0.33
mlp_output_dim_arc = 500
mlp_output_dim_rel = 100
mlp_input_dropout_ratio = 0.33
mlp_output_dropout_ratio = 0.33

[Optimizer]
learning_rate = 2e-3
decay = .75
decay_steps = 5000
beta_1 = .9
beta_2 = .9
epsilon = 1e-12
clip = 5.0

[Run]
is_test = 0
is_train = 1
threads = 8
inst_num_max = -1
max_bucket_num = 72
max_sent_length = 2500
sent_batch_size = 10
char_batch_sizes = 500, 500, 500, 500
min_word_len = 2,
max_word_len = 5,
with_bert = 0
with_extra_dictionarys = 0

[Test]
model_eval_num = 16

[Train]
path = "exp/sighan10/"
train_files = "data/sighan10/train.conll", 
dev_files = "data/sighan10/dev.conll", 
test_files = "data/sighan10/com_test.conll", "data/sighan10/fin_test.conll", "data/sighan10/med_test.conll", "data/sighan10/lit_test.conll"
train_extra_dictionarys = "data/dict/dict_1",
dev_extra_dictionarys = "data/dict/dict_1",
test_extra_dictionarys = "data/dict/dict_1",
bert_path = "data/bert-base-chinese.tar.gz"
bert_vocab = "data/bert-base-chinese-vocab.txt"
bert_config = "data/bert_config.json"
is_dictionary_exist = 1
train_max_eval_num = 1000
cutoff_freq = 1
patience = 10

[Network]
n_char_embed = 50
n_dict_embed = 5,
n_bert_layers = 4
n_bert_embed = 100
embed_dropout = 0.5
n_lstm_hidden = 150
n_lstm_layers = 2
lstm_dropout = 0.2

[Optimizer]
lr = 1e-3
clip = 5.0
